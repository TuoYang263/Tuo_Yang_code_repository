{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"train_other_network.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1kzw0o4vfUs0yF7O2pSmLwGsi9hmFGLZp","authorship_tag":"ABX9TyO7/eSGd5dzfc/0tDwW+mU3"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"yvbQvNPXGUhd"},"source":["!pip install tensorflow-gpu"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QhznFtwWyYOn"},"source":["# **DRIVE dataset**"]},{"cell_type":"code","metadata":{"id":"M7WzVEbGlWE2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwrDFho17O-E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620131548225,"user_tz":-180,"elapsed":1243,"user":{"displayName":"DUO YANG","photoUrl":"","userId":"14621909467358785593"}},"outputId":"c7ae69aa-ad22-4516-fe7b-7e9408b8dff6"},"source":["np.linalg.norm(x=grads[0].numpy().flatten(), ord=2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6011937"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"t4BCRqD5yjnw"},"source":["# **HRF DATASET**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ctnObbcQt3KaDBQ2oWBTEO5ZWNyI-YOd"},"id":"lhRYSvJ4yo0P","outputId":"ea3660b6-83b5-4de0-ecde-dda2b514aa23"},"source":["#margin sampling\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import sys\n","import random\n","import numpy as np\n","import heapq\n","import pickle\n","\n","from google.colab import drive\n","drive.mount('/content/drive') #authorize google drive\n","sys.path.append('/content/drive/MyDrive/MyProject/') #the storage path of my own modules\n","from keras.callbacks import EarlyStopping\n","from util import *\n","from unet import *\n","from pre_processing import *\n","from sklearn.model_selection import train_test_split\n","from keras import backend as K\n","\n","def margin_sampling(probability_map1, probability_map2):\n","  maximum_probs = np.maximum(probability_map1, probability_map2)\n","  minimum_probs = np.minimum(probability_map1, probability_map2)\n","  deduction = maximum_probs - minimum_probs\n","  #print(deduction.shape)\n","  #calculate the Euclidean Distance\n","  return np.round(np.sqrt(np.sum(np.square(deduction))),6)\n","\n","def least_confidence(probability_map1, probability_map2):\n","  #next step is equal to 1 - np.minimumprobability_map1, probability_map2)\n","  least_confidence = 1 - np.maximum(probability_map1, probability_map2)\n","  #calculate the Euclidean Distance \n","  return np.round(np.sqrt(np.sum(np.square(least_confidence))),6)\n","\n","def entropy_sampling(probability_map1, probability_map2):\n","  #base = 10\n","  item1 = np.dot(probability_map1,np.log10(probability_map1))\n","  item2 = np.dot(probability_map2,np.log10(probability_map2))\n","  sum_of_item = -(item1+item2)\n","  #calculate the Euclidean Distance\n","  return np.round(np.sqrt(np.sum(np.square(sum_of_item))),6)\n","\n","def EGL(probability_map1, probability_map2, grads_norm):\n","  item1 = probability_map1 * grads_norm\n","  item2 = probability_map2 * grads_norm\n","  item = item1 + item2\n","  return np.mean(item)\n","\n","def tversky(y_true, y_pred, smooth=1):\n","  y_true_pos = K.flatten(y_true)\n","  y_pred_pos = K.flatten(y_pred)\n","  true_pos = K.sum(y_true_pos * y_pred_pos)\n","  false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n","  false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n","  alpha = 0.5\n","  return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n","\n","def tversky_loss(y_true, y_pred):\n","  return 1 - tversky(y_true, y_pred, smooth=1)\n","  \n","if __name__ == '__main__':\n","\n","  # parameters and paths\n","  # DRIVE resize:(576, 576)\n","  # HRF resize:(2920, 2920)\n","  # IOSTAR resize:(1024, 1024)\n","  resize_height, resize_width = (1472, 2176)\n","  # DRIVE patch size:48\n","  # HRF patch size:292\n","  # IOSTAR resize:64\n","  dx = 64\n","  # dataset option: DRIVE, HRF and IOSTAR\n","  dataset = \"HRF\"\n","\n","  img_path = '/content/drive/MyDrive/MyProject/'+dataset+'/training/images/'\n","  label_path = '/content/drive/MyDrive/MyProject/'+dataset+'/training/1st_manual/'\n","\n","  # read the data and resize them\n","  imglst,images = read_image_and_name(img_path)\n","  labellst,labels = read_label_and_name(label_path)\n","  \n","  # pre selected annotated samples by active learning(inside 20 training images)\n","  # range [2, 6, 10, 14, 18, 20] , 6, 10, 14, 18 \n","  selected_image_nums = [2, 6, 10, 14, 18, 20]  #it could be gained by percent as well, like int(0.5*len(data))\n","\n","  #set the name of method will be used, selective options\n","  #margin_sampling, least_confidence, entropy_sampling, excepted_gradient_length, random_sampling\n","  #\"least_confidence\", \"entropy_sampling\", \"excepted_gradient_length\", , \"random_sampling\"  \"margin_sampling\", \"least_confidence\", \"entropy_sampling\", \"excepted_gradient_length\",\n","  methods = [\"margin_sampling\", \"least_confidence\", \"entropy_sampling\", \"excepted_gradient_length\", \"random_sampling\"]\n","\n","  # randomly selected N(stored inside the variable imglst) samples from the training dataset\n","  # due to the uncertainty of accuracy from the initialized classifier, take labeled data in advance\n","  selected_data_set = []\n","  selected_label_set = []\n","  sample_index_set = []\n","  image_index = [i for i in range(len(images))]\n","  for selected_image_num in selected_image_nums:\n","    sample_index = random.sample(image_index, selected_image_num)\n","    sample_data = [images[i] for i in sample_index]\n","    sample_label = [labels[i] for i in sample_index]\n","    selected_data_set.append(sample_data)\n","    selected_label_set.append(sample_label)\n","    sample_index_set.append(sample_index)\n","\n","  for method in methods:\n","    for selected_index, selected_item in enumerate(selected_image_nums):\n","      sample_data = selected_data_set[selected_index]\n","      sample_label = selected_label_set[selected_index]\n","      sample_index = sample_index_set[selected_index]\n","      \n","      #check if the order of images and labels is not corresponding\n","      plt.figure()\n","      plt.imshow(sample_data[1])\n","      plt.show()\n","\n","      plt.figure()\n","      plt.imshow(sample_label[1],'gray')\n","      plt.show()\n","\n","      imgs_resize = resize(sample_data,resize_width,resize_height)\n","      labels_resize = resize(sample_label,resize_width,resize_height)\n","\n","      # convert the list imgs and manuals to the numpy array\n","      X_train = np.array(imgs_resize)\n","      Y_train = np.array(labels_resize)\n","      print(X_train.shape)\n","      print(Y_train.shape)\n","\n","\n","      # do the standardization to the image, each pixel value will be\n","      # restricted within 0 to 1\n","      X_train = X_train.astype('float32')/255\n","      Y_train = Y_train.astype('float32')/255\n","\n","      # do the pre-processing operations to the training images\n","      X_train = my_PreProc(X_train)\n","\n","      print(X_train.shape)\n","      plt.figure()\n","      plt.imshow(X_train[1,:,:,0],'gray')\n","      plt.show()\n","\n","      plt.figure()\n","      plt.imshow(Y_train[1,:,:],'gray')\n","      plt.show()\n","\n","      print('X_train shape: ' + str(X_train.shape))  # X_train(2880,48,48)\n","      print('Y_train shape: ' + str(Y_train.shape))  # Y_train(2880,48,48)\n","\n","      # crop training images\n","      patches_num = 300;       #required patches num 1000\n","\n","      X_train, Y_train = extract_random(X_train, Y_train, dx, dx, patches_num*selected_image_num)\n","      print('X_train shape: '+str(X_train.shape)) #X_train(2880,48,48)\n","      print('Y_train shape: '+str(Y_train.shape)) #Y_train(2880,48,48)\n","      \n","      \"\"\"\n","      #Save a sample of what you're feeding to the neural network\n","      N_sample = min(X_train.shape[0],40)\n","      temp_Ytrain = Y_train[...,np.newaxis]\n","      #visualize(,'/content/drive/MyDrive/MyProject/'+dataset+'/samples/sample_input_imgs')\n","      visualize(group_images(X_train[0:N_sample,:,:,:],5),'/content/drive/MyDrive/MyProject/'+dataset+'/samples/sample_input_imgs')\n","      visualize(group_images(temp_Ytrain[0:N_sample,:,:,:],5),'/content/drive/MyDrive/MyProject/'+dataset+'/samples/sample_output_imgs')\n","      \"\"\"\n","\n","      plt.figure()\n","      plt.imshow(X_train[12,:,:,0],'gray')\n","      plt.show()\n","\n","      plt.figure()\n","      plt.imshow(Y_train[12,:,:],'gray')\n","      plt.show()\n","      \n","      # Increase one dimension to X_train, the dimension will become (2880,1,48,48)\n","      X_train = X_train[:,np.newaxis, ...]\n","      print('X_train shape: '+str(X_train.shape))\n","      # change the shape of Y_train, make it become (2880,2304), keep the first dimension\n","      # unchanged, merge other dimensions\n","      Y_train = Y_train.reshape(Y_train.shape[0],-1)\n","      print('Y_train shape: '+str(Y_train.shape))\n","      # increase one dimension, make it become (2880,2304,1)\n","      Y_train =Y_train[..., np.newaxis]\n","      print('Y_train shape: '+str(Y_train.shape))\n","      temp = 1 - Y_train\n","      # make it become (2880,2304,2)\n","      Y_train = np.concatenate([Y_train, temp], axis=2)\n","      print('Y_train shape: '+str(Y_train.shape))\n","      print('X_train shape: '+str(X_train.shape))\n","      \n","      \n","      # model is gained\n","      n_label = 2   #number of classes\n","      model = None\n","      model = get_unet(X_train.shape[1],X_train.shape[2],X_train.shape[3],n_label)\n","\n","      #set early stopping to prevent overfit\n","      early_stopping = EarlyStopping(monitor='loss',patience=3,verbose=2)\n","      \n","      model.summary()  # output the calculation process of parameter Param\n","      checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/MyProject/'+dataset+'/training_records/'+dataset+'_best_weights('+method+'_'+str(selected_item)+\n","                                      ').h5',verbose=1,monitor='val_accuracy',mode='auto',save_best_only=True)\n","      \n","      model.compile(optimizer=Adam(lr=0.0005),loss='categorical_crossentropy',metrics=['accuracy']) \n","\n","\n","      #split 80% for training and 20% for validation\n","      seed = 7\n","      np.random.seed(seed)\n","      X_trainset, X_testset, Y_trainset, Y_testset = train_test_split(X_train, Y_train, test_size=0.2, random_state=seed)\n","\n","      history1 = model.fit(X_trainset,Y_trainset,batch_size=64,epochs=10,verbose=2,shuffle=True,validation_data=(X_testset, Y_testset),callbacks=[checkpointer,early_stopping])\n","      \n","      # use the uncertainty and fine-tuning to train this model\n","      training_samples_num = selected_item * patches_num\n","      # patches number extracted from unlabeled images\n","      unlabeled_patches_num = 100\n","\n","      active_indice = [i for i in image_index if i not in sample_index]\n","      print(\"active_indice:\"+str(len(active_indice)))\n","\n","      final_new_train_samples = X_trainset\n","      final_new_train_labels = Y_trainset\n","\n","      epochs = 0\n","\n","      #start the for-loop from here\n","      #while len(active_indice) != 0:\n","      #a = 1\n","      #while a == 1:\n","\n","      #a = 1\n","      while len(active_indice) != 0:\n","          \n","        active_image_data = [images[i] for i in active_indice]\n","        active_image_label = [labels[i] for i in active_indice]\n","\n","        active_imgs_resize = resize(active_image_data,resize_width,resize_height)\n","        active_labels_resize = resize(active_image_label,resize_width,resize_height)\n","\n","        #extract patches from unlabeled images again, take their labels meanwhile\n","        #convert the list imgs and manuals to the numpy array\n","        active_X_train = np.array(active_imgs_resize)\n","        active_Y_train = np.array(active_labels_resize)\n","\n","        # do the standardization to the image, each pixel value will be\n","        # restricted within 0 to 1\n","        active_X_train = active_X_train.astype('float32')/255\n","        active_Y_train = active_Y_train.astype('float32')/255\n","\n","        # do the pre-processing operations to the training images\n","        active_X_train = my_PreProc(active_X_train)\n","        print('active_X_train shape: '+str(active_X_train.shape)) \n","        print('active_Y_train shape: '+str(active_Y_train.shape))\n","        \n","        sample_confidence = []\n","        if method is not \"random_sampling\":\n","          #active_samples_num = len(active_indice) * unlabeled_patches_num\n","          for k in range(active_X_train.shape[0]):   \n","            list1 = []\n","            list2 = []\n","            for i in range(resize_height//dx):\n","              for j in range(resize_width//dx):\n","                list1.append(active_X_train[k,...][i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n","                list2.append(active_Y_train[k,...][i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n","            \n","            active_prediction_samples = np.array(list1)[:,np.newaxis,...] #increase one dimension, it will become (144,1,48,48)\n","            active_groundtruth_samples = np.array(list2)[:,np.newaxis,...]\n","            #print('active_X_train shape: '+str(active_prediction_samples.shape)) \n","            #print('active_Y_train shape: '+str(active_Y_train.shape))\n","\n","            #print(new_X_train.shape)\n","            Y_pred_train = model.predict(active_prediction_samples)\n","            #print('predict shape: '+str(Y_pred_train.shape))\n","\n","            Y_pred1 = Y_pred_train[...,0]\n","            Y_pred2 = Y_pred_train[...,1]\n","\n","            probability_map1 = np.zeros((resize_height, resize_width))\n","            probability_map2 = np.zeros((resize_height, resize_width))\n","\n","            t = 0\n","            for i in range(resize_height//dx):\n","              for j in range(resize_width//dx):\n","                temp1 = Y_pred1[t].reshape(dx, dx)\n","                temp2 = Y_pred2[t].reshape(dx, dx)\n","                probability_map1[i*dx:(i+1)*dx, j*dx:(j+1)*dx] = temp1\n","                probability_map2[i*dx:(i+1)*dx, j*dx:(j+1)*dx] = temp2\n","                t = t + 1\n","            \n","            probability_map1 = cv2.resize(probability_map1,(resize_height,resize_width))\n","            probability_map2 = cv2.resize(probability_map2,(resize_height,resize_width))\n","            \n","            #output one to see the probability\n","            #print('probability:'+str(Y_pred_train[1,:,0]))\n","            #print('probability:'+str(Y_pred_train[1,:,1]))\n","\n","            #calculate every unannotated image's uncertainty\n","            #S = np.round(compute_matrix(Y_pred_train, 0, 1, 1),2)\n","            if method == \"margin_sampling\":\n","              S = margin_sampling(probability_map1, probability_map2)\n","            elif method == \"least_confidence\":\n","              S = least_confidence(probability_map1, probability_map2)\n","            elif method == \"entropy_sampling\":\n","              S = entropy_sampling(probability_map1, probability_map2)\n","            elif method == \"excepted_gradient_length\":\n","              real_y = active_groundtruth_samples\n","              real_y = real_y.reshape(real_y.shape[0],-1)\n","              real_y = real_y[...,np.newaxis]\n","              temp = 1 - real_y\n","              real_y = np.concatenate([real_y, temp], axis=2)\n","              \n","              with tf.GradientTape() as tape:\n","                cce = tf.keras.losses.CategoricalCrossentropy()\n","                pred_y = model(active_prediction_samples)\n","                ce_loss = cce(real_y, pred_y)\n","              \n","              grads = tape.gradient(ce_loss,model.trainable_variables)\n","              grads_norm = 0\n","              #print(len(grads))\n","              for i in range(len(grads)):\n","                grads_norm = grads_norm + np.linalg.norm(x=grads[i].numpy().flatten(),ord=2)\n","              #print(grads_norm)\n","              S = EGL(probability_map1, probability_map2, grads_norm)\n","\n","            sample_confidence.append(S)\n","            \n","        #print(active_scores)\n","        print(sample_confidence)\n","\n","        #sample_confidence:convert list to array\n","        sample_confidence = np.array(sample_confidence)\n","        \n","        #smallest_index = [index for index,value in enumerate(sample_confidence) if value==min(sample_confidence)]\n","        #find the elements with lowest/highest active scores\n","        smallest_index = []\n","        selected_num = 2 #2 3\n","        if method == \"random_sampling\":\n","          current_active_indice = [i for i in range(len(active_indice))];\n","          if len(current_active_indice)<selected_num:\n","            smallest_index = random.sample(current_active_indice, len(current_active_indice))\n","          else:\n","            smallest_index = random.sample(current_active_indice, selected_num)\n","        elif method == \"margin_sampling\":\n","          sorts = np.argsort(sample_confidence)\n","          if len(sorts) < selected_num:\n","            smallest_index = sorts\n","          else:\n","            smallest_index = sorts[0:selected_num]\n","        elif method == \"least_confidence\":\n","          sorts = np.argsort(-sample_confidence)\n","          if len(sorts) < selected_num:\n","            smallest_index = sorts\n","          else:\n","            smallest_index = sorts[0:selected_num]\n","        elif method == \"entropy_sampling\":\n","          sorts = np.argsort(-sample_confidence)\n","          if len(sorts) < selected_num:\n","            smallest_index = sorts\n","          else:\n","            smallest_index = sorts[0:selected_num]\n","        elif method == \"excepted_gradient_length\":\n","          sorts = np.argsort(-sample_confidence)\n","          if len(sorts) < selected_num:\n","            smallest_index = sorts\n","          else:\n","            smallest_index = sorts[0:selected_num]\n","        \n","        print(smallest_index)\n","        \n","        smallest_elements = [active_X_train[i] for i in smallest_index]\n","        smallest_labels = [active_Y_train[i] for i in smallest_index]\n","\n","        reshaped_elements = []\n","        reshaped_labels = []\n","        for tuning_item, tuning_label in zip(smallest_elements, smallest_labels):\n","          item = [tuning_item]\n","          label = [tuning_label]\n","          tuning_item = resize(item,resize_width,resize_height)\n","          tuning_label = resize(label,resize_width,resize_height)\n","\n","          tuning_item = np.array(tuning_item)\n","          tuning_label = np.array(tuning_label)\n","\n","          #smallest_elements, smallest_labels = extract_random(smallest_elements, smallest_labels, dx, dx, unlabeled_patches_num*len(smallest_index))\n","          \n","          temp_elements = []\n","          temp_labels = []\n","          for a, b in zip(tuning_item, tuning_label):\n","            for i in range(resize_height//dx):\n","              for j in range(resize_width//dx):\n","                temp_elements.append(a[i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n","                temp_labels.append(b[i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n","\n","          temp_elements = np.array(temp_elements)[:,np.newaxis,...,np.newaxis]\n","          temp_labels = np.array(temp_labels)[:,...,np.newaxis]\n","\n","          reshaped_elements.append(temp_elements)\n","          reshaped_labels.append(temp_labels)\n","\n","        smallest_elements = np.concatenate(reshaped_elements, axis=0)\n","        smallest_labels = np.concatenate(reshaped_labels, axis=0)\n","        print(smallest_elements.shape)\n","        print(smallest_labels.shape)\n","\n","        #smallest_elements = smallest_elements[..., np.newaxis]\n","\n","        # change the shape of Y_train, make it become (2880,2304), keep the first dimension\n","        # unchanged, merge other dimensions\n","        smallest_labels = smallest_labels.reshape(smallest_labels.shape[0],-1)\n","        print('smallest_labels shape: '+str(smallest_labels.shape))\n","        # increase one dimension, make it become (2880,2304,1)\n","        smallest_labels = smallest_labels[..., np.newaxis]\n","        print('Y_train shape: '+str(Y_train.shape))\n","        temp = 1 - smallest_labels\n","        # make it become (2880,2304,2)\n","        smallest_labels = np.concatenate([smallest_labels, temp], axis=2)\n","\n","        print(smallest_elements.shape)\n","        print(smallest_labels.shape)          \n","        \n","\n","        new_train_samples = []\n","        for i in range(final_new_train_samples.shape[0]):\n","          ori_element = final_new_train_samples[i,...]\n","          new_train_samples.append(ori_element[np.newaxis,:])\n","        for j in range(smallest_elements.shape[0]):\n","          new_element = smallest_elements[j,...]\n","          new_train_samples.append(new_element[np.newaxis,:])\n","        final_new_train_samples = np.concatenate(new_train_samples,axis=0)\n","        \n","        new_train_labels = []\n","        for i in range(final_new_train_labels.shape[0]):\n","          ori_label = final_new_train_labels[i,...]\n","          new_train_labels.append(ori_label[np.newaxis,:])\n","        for j in range(smallest_labels.shape[0]):\n","          new_label = smallest_labels[j,...]\n","          new_train_labels.append(new_label[np.newaxis,:])\n","        final_new_train_labels = np.concatenate(new_train_labels,axis=0)\n","\n","        print('final_new_train_samples shape: '+str(final_new_train_samples.shape))\n","        print('final_new_train_labels shape: '+str(final_new_train_labels.shape))\n","      \n","        # use these 100 the most uncertain samples \n","        # to do the fine-tuning to this network\n","        print(\"Use the pre trained model for fine tuning\")\n","        print(model.layers)\n","\n","        for layer in model.layers[:10]:\n","          layer.trainable = False\n","          #print(layer)\n","          #print(\"these layers won't be trained\")\n","        for layer in model.layers[21:30]:\n","          layer.trainable = False\n","          #print(layer)\n","          #print(\"these layers won't be trained\")\n","        \n","        print(len(model.layers))\n","\n","        #set early stopping to prevent overfit\n","        #early_stopping = EarlyStopping(monitor='val_loss',patience=3,verbose=2)\n","\n","        #model.summary()  # output the calculation process of parameter Param\n","        checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/MyProject/'+dataset+'/training_records/'+dataset+'_best_weights('+method+'_'+str(selected_item)+\n","                                      ').h5',verbose=1,monitor='accuracy',mode='auto',save_best_only=True)\n","        model.compile(optimizer=Adam(lr=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])\n","        history2 = model.fit(final_new_train_samples,final_new_train_labels,batch_size=64,epochs=10,verbose=2,shuffle=True,validation_split=0.0,callbacks=[checkpointer,early_stopping]) #,early_stopping\n","\n","        #update corresponding parameters\n","        ruleout_list = [active_indice[i] for i in smallest_index]\n","        active_indice = [x for x in active_indice if x not in ruleout_list]\n","        print(active_indice)\n","        active_samples_num = len(active_indice)*patches_num\n","\n","        # draw corresponding loss figures\n","        history1.history['accuracy'].extend(history2.history['accuracy'])\n","        history1.history['loss'].extend(history2.history['loss'])\n","        \n","      #print(history1.history['accuracy'])\n","      #print(history1.history['loss'])\n","\n","      #mode wb+ readable and writable, and files will be automatically created if they don't exist\n","      #binary mode\n","      with open('/content/drive/MyDrive/MyProject/'+dataset+'/training_records/'+method+'_'+str(selected_item)+'.txt','wb+') as file_pi:\n","        pickle.dump(history1.history, file_pi)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"U62BbyMobmxp"},"source":["# **CHASE_DB**"]},{"cell_type":"code","metadata":{"id":"y9iIGyTKRhwi"},"source":["#margin sampling\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import sys\n","import random\n","import numpy as np\n","import heapq\n","import pickle\n","\n","from google.colab import drive\n","drive.mount('/content/drive') #authorize google drive\n","sys.path.append('/content/drive/MyDrive/MyProject/') #the storage path of my own modules\n","from keras.callbacks import EarlyStopping\n","from util import *\n","from unet import *\n","from pre_processing import *\n","from sklearn.model_selection import train_test_split\n","from keras import backend as K\n","\n","def margin_sampling(probability_map1, probability_map2):\n","  maximum_probs = np.maximum(probability_map1, probability_map2)\n","  minimum_probs = np.minimum(probability_map1, probability_map2)\n","  deduction = maximum_probs - minimum_probs\n","  #print(deduction.shape)\n","  #calculate the Euclidean Distance\n","  return np.round(np.sqrt(np.sum(np.square(deduction))),6)\n","\n","def least_confidence(probability_map1, probability_map2):\n","  #next step is equal to 1 - np.minimumprobability_map1, probability_map2)\n","  least_confidence = 1 - np.maximum(probability_map1, probability_map2)\n","  #calculate the Euclidean Distance \n","  return np.round(np.sqrt(np.sum(np.square(least_confidence))),6)\n","\n","def entropy_sampling(probability_map1, probability_map2):\n","  #base = 10\n","  item1 = np.dot(probability_map1,np.log10(probability_map1))\n","  item2 = np.dot(probability_map2,np.log10(probability_map2))\n","  sum_of_item = -(item1+item2)\n","  #calculate the Euclidean Distance\n","  return np.round(np.sqrt(np.sum(np.square(sum_of_item))),6)\n","\n","def EGL(probability_map1, probability_map2, grads_norm):\n","  item1 = probability_map1 * grads_norm\n","  item2 = probability_map2 * grads_norm\n","  item = item1 + item2\n","  return np.round(np.sqrt(np.sum(np.square(item))),6)\n","\n","def tversky(y_true, y_pred, smooth=1):\n","  y_true_pos = K.flatten(y_true)\n","  y_pred_pos = K.flatten(y_pred)\n","  true_pos = K.sum(y_true_pos * y_pred_pos)\n","  false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n","  false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n","  alpha = 0.5\n","  return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n","\n","def tversky_loss(y_true, y_pred):\n","  return 1 - tversky(y_true, y_pred, smooth=1)\n","  \n","if __name__ == '__main__':\n","\n","  # parameters and paths\n","  # DRIVE resize:(576, 576)\n","  # HRF resize:(2920, 2920)\n","  # IOSTAR resize:(1024, 1024)\n","  resize_height, resize_width = (992, 992)\n","  # DRIVE patch size:48\n","  # HRF patch size:292\n","  # IOSTAR resize:64\n","  dx = 32\n","  # dataset option: DRIVE, HRF and IOSTAR\n","  dataset = \"CHASE\"\n","\n","  img_path = '/content/drive/MyDrive/MyProject/'+dataset+'/training/images/'\n","  label_path = '/content/drive/MyDrive/MyProject/'+dataset+'/training/1st_manual/'\n","\n","  # read the data and resize them\n","  imglst,images = read_image_and_name(img_path)\n","  labellst,labels = read_label_and_name(label_path)\n","  \n","  # pre selected annotated samples by active learning(inside 20 training images)\n","  # range [2, 6, 10, 14, 18, 20]  [2, 6, 10, 14, 18]\n","  selected_image_nums = [2]  #it could be gained by percent as well, like int(0.5*len(data))\n","\n","  #set the name of method will be used, selective options\n","  #margin_sampling, least_confidence, entropy_sampling, excepted_gradient_length, random_sampling\n","  #\"random_sampling\"  \"margin_sampling\", \"least_confidence\", \"entropy_sampling\", \"excepted_gradient_length\", \n","  methods = [\"margin_sampling\"]\n","\n","  # randomly selected N(stored inside the variable imglst) samples from the training dataset\n","  # due to the uncertainty of accuracy from the initialized classifier, take labeled data in advance\n","  selected_data_set = []\n","  selected_label_set = []\n","  sample_index_set = []\n","  image_index = [i for i in range(len(images))]\n","  for selected_image_num in selected_image_nums:\n","    sample_index = random.sample(image_index, selected_image_num)\n","    sample_data = [images[i] for i in sample_index]\n","    sample_label = [labels[i] for i in sample_index]\n","    selected_data_set.append(sample_data)\n","    selected_label_set.append(sample_label)\n","    sample_index_set.append(sample_index)\n","\n","  for method in methods:\n","    for selected_index, selected_item in enumerate(selected_image_nums):\n","      sample_data = selected_data_set[selected_index]\n","      sample_label = selected_label_set[selected_index]\n","      sample_index = sample_index_set[selected_index]\n","      \n","      #check if the order of images and labels is not corresponding\n","      plt.figure()\n","      plt.imshow(sample_data[1])\n","      plt.show()\n","\n","      plt.figure()\n","      plt.imshow(sample_label[1],'gray')\n","      plt.show()\n","\n","      imgs_resize = resize(sample_data,resize_width,resize_height)\n","      labels_resize = resize(sample_label,resize_width,resize_height)\n","\n","      # convert the list imgs and manuals to the numpy array\n","      X_train = np.array(imgs_resize)\n","      Y_train = np.array(labels_resize)\n","      print(X_train.shape)\n","      print(Y_train.shape)\n","\n","\n","      # do the standardization to the image, each pixel value will be\n","      # restricted within 0 to 1\n","      X_train = X_train.astype('float32')/255\n","      Y_train = Y_train.astype('float32')/255\n","\n","      # do the pre-processing operations to the training images\n","      X_train = my_PreProc(X_train)\n","\n","      print(X_train.shape)\n","      plt.figure()\n","      plt.imshow(X_train[1,:,:,0],'gray')\n","      plt.show()\n","\n","      plt.figure()\n","      plt.imshow(Y_train[1,:,:],'gray')\n","      plt.show()\n","\n","      print('X_train shape: ' + str(X_train.shape))  # X_train(2880,48,48)\n","      print('Y_train shape: ' + str(Y_train.shape))  # Y_train(2880,48,48)\n","\n","      # crop training images\n","      patches_num = 600;       #required patches num 1000\n","\n","      X_train, Y_train = extract_random(X_train, Y_train, dx, dx, patches_num*selected_image_num)\n","      print('X_train shape: '+str(X_train.shape)) #X_train(2880,48,48)\n","      print('Y_train shape: '+str(Y_train.shape)) #Y_train(2880,48,48)\n","      \n","      \n","      #Save a sample of what you're feeding to the neural network\n","      N_sample = min(X_train.shape[0],40)\n","      temp_Ytrain = Y_train[...,np.newaxis]\n","      #visualize(,'/content/drive/MyDrive/MyProject/'+dataset+'/samples/sample_input_imgs')\n","      visualize(group_images(X_train[0:N_sample,:,:,:],5),'/content/drive/MyDrive/MyProject/'+dataset+'/samples/sample_input_imgs')\n","      visualize(group_images(temp_Ytrain[0:N_sample,:,:,:],5),'/content/drive/MyDrive/MyProject/'+dataset+'/samples/sample_output_imgs')\n","\n","\n","      plt.figure()\n","      plt.imshow(X_train[12,:,:,0],'gray')\n","      plt.show()\n","\n","      plt.figure()\n","      plt.imshow(Y_train[12,:,:],'gray')\n","      plt.show()\n","\n","      \"\"\"\n","      \n","      # Increase one dimension to X_train, the dimension will become (2880,1,48,48)\n","      X_train = X_train[:,np.newaxis, ...]\n","      print('X_train shape: '+str(X_train.shape))\n","      # change the shape of Y_train, make it become (2880,2304), keep the first dimension\n","      # unchanged, merge other dimensions\n","      Y_train = Y_train.reshape(Y_train.shape[0],-1)\n","      print('Y_train shape: '+str(Y_train.shape))\n","      # increase one dimension, make it become (2880,2304,1)\n","      Y_train =Y_train[..., np.newaxis]\n","      print('Y_train shape: '+str(Y_train.shape))\n","      temp = 1 - Y_train\n","      # make it become (2880,2304,2)\n","      Y_train = np.concatenate([Y_train, temp], axis=2)\n","      print('Y_train shape: '+str(Y_train.shape))\n","      print('X_train shape: '+str(X_train.shape))\n","      \n","      \n","      # model is gained\n","      n_label = 2   #number of classes\n","      model = None\n","      model = get_unet(X_train.shape[1],X_train.shape[2],X_train.shape[3],n_label)\n","\n","      #set early stopping to prevent overfit\n","      early_stopping = EarlyStopping(monitor='loss',patience=3,verbose=2)\n","      \n","      model.summary()  # output the calculation process of parameter Param\n","      checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/MyProject/'+dataset+'/training_records/'+dataset+'_best_weights('+method+'_'+str(selected_item)+\n","                                      ').h5',verbose=1,monitor='val_accuracy',mode='auto',save_best_only=True)\n","      \n","      model.compile(optimizer=Adam(lr=0.0005),loss='categorical_crossentropy',metrics=['accuracy']) \n","\n","\n","      #split 80% for training and 20% for validation\n","      seed = 7\n","      np.random.seed(seed)\n","      X_trainset, X_testset, Y_trainset, Y_testset = train_test_split(X_train, Y_train, test_size=0.2, random_state=seed)\n","\n","      history1 = model.fit(X_trainset,Y_trainset,batch_size=64,epochs=10,verbose=2,shuffle=True,validation_data=(X_testset, Y_testset),callbacks=[checkpointer,early_stopping])\n","      \n","      # use the uncertainty and fine-tuning to train this model\n","      training_samples_num = selected_item * patches_num\n","      # patches number extracted from unlabeled images\n","      unlabeled_patches_num = 100\n","\n","      active_indice = [i for i in image_index if i not in sample_index]\n","      print(\"active_indice:\"+str(len(active_indice)))\n","\n","      final_new_train_samples = X_trainset\n","      final_new_train_labels = Y_trainset\n","\n","      epochs = 0\n","\n","      #start the for-loop from here\n","      #while len(active_indice) != 0:\n","      #a = 1\n","      #while a == 1:\n","\n","      #a = 1\n","      while len(active_indice) != 0:\n","          \n","        active_image_data = [images[i] for i in active_indice]\n","        active_image_label = [labels[i] for i in active_indice]\n","\n","        active_imgs_resize = resize(active_image_data,resize_width,resize_height)\n","        active_labels_resize = resize(active_image_label,resize_width,resize_height)\n","\n","        #extract patches from unlabeled images again, take their labels meanwhile\n","        #convert the list imgs and manuals to the numpy array\n","        active_X_train = np.array(active_imgs_resize)\n","        active_Y_train = np.array(active_labels_resize)\n","\n","        # do the standardization to the image, each pixel value will be\n","        # restricted within 0 to 1\n","        active_X_train = active_X_train.astype('float32')/255\n","        active_Y_train = active_Y_train.astype('float32')/255\n","\n","        # do the pre-processing operations to the training images\n","        active_X_train = my_PreProc(active_X_train)\n","        print('active_X_train shape: '+str(active_X_train.shape)) \n","        print('active_Y_train shape: '+str(active_Y_train.shape))\n","        \n","        sample_confidence = []\n","        if method is not \"random_sampling\":\n","          #active_samples_num = len(active_indice) * unlabeled_patches_num\n","          for k in range(active_X_train.shape[0]):   \n","            list1 = []\n","            list2 = []\n","            for i in range(resize_height//dx):\n","              for j in range(resize_width//dx):\n","                list1.append(active_X_train[k,...][i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n","                list2.append(active_Y_train[k,...][i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n","            active_prediction_samples = np.array(list1)[:,np.newaxis,...] #increase one dimension, it will become (144,1,48,48)\n","            active_groundtruth_samples = np.array(list2)[:,np.newaxis,...]\n","            #print('active_X_train shape: '+str(active_prediction_samples.shape)) \n","            #print('active_Y_train shape: '+str(active_Y_train.shape))\n","\n","            #print(new_X_train.shape)\n","            Y_pred_train = model.predict(active_prediction_samples)\n","            #print('predict shape: '+str(Y_pred_train.shape))\n","\n","            Y_pred1 = Y_pred_train[...,0]\n","            Y_pred2 = Y_pred_train[...,1]\n","\n","            probability_map1 = np.zeros((resize_height, resize_width))\n","            probability_map2 = np.zeros((resize_height, resize_width))\n","\n","            t = 0\n","            for i in range(resize_height//dx):\n","              for j in range(resize_width//dx):\n","                temp1 = Y_pred1[t].reshape(dx, dx)\n","                temp2 = Y_pred2[t].reshape(dx, dx)\n","                probability_map1[i*dx:(i+1)*dx, j*dx:(j+1)*dx] = temp1\n","                probability_map2[i*dx:(i+1)*dx, j*dx:(j+1)*dx] = temp2\n","                t = t + 1\n","            \n","            probability_map1 = cv2.resize(probability_map1,(resize_height,resize_width))\n","            probability_map2 = cv2.resize(probability_map2,(resize_height,resize_width))\n","            \n","            #output one to see the probability\n","            #print('probability:'+str(Y_pred_train[1,:,0]))\n","            #print('probability:'+str(Y_pred_train[1,:,1]))\n","\n","            #calculate every unannotated image's uncertainty\n","            #S = np.round(compute_matrix(Y_pred_train, 0, 1, 1),2)\n","            if method == \"margin_sampling\":\n","              S = margin_sampling(probability_map1, probability_map2)\n","            elif method == \"least_confidence\":\n","              S = least_confidence(probability_map1, probability_map2)\n","            elif method == \"entropy_sampling\":\n","              S = entropy_sampling(probability_map1, probability_map2)\n","            elif method == \"excepted_gradient_length\":\n","              real_y = active_groundtruth_samples\n","              real_y = real_y.reshape(real_y.shape[0],-1)\n","              real_y = real_y[...,np.newaxis]\n","              temp = 1 - real_y\n","              real_y = np.concatenate([real_y, temp], axis=2)\n","              \n","              with tf.GradientTape() as tape:\n","                cce = tf.keras.losses.CategoricalCrossentropy()\n","                pred_y = model(active_prediction_samples)\n","                ce_loss = cce(real_y, pred_y)\n","              \n","              grads = tape.gradient(ce_loss,model.trainable_variables)\n","              grads_norm = 0\n","              #print(len(grads))\n","              for i in range(len(grads)):\n","                grads_norm = grads_norm + np.linalg.norm(x=grads[i].numpy().flatten(),ord=2)\n","              #print(grads_norm)\n","              S = EGL(probability_map1, probability_map2, grads_norm)\n","\n","            sample_confidence.append(S)\n","            \n","        #print(active_scores)\n","        print(sample_confidence)\n","\n","        #sample_confidence:convert list to array\n","        sample_confidence = np.array(sample_confidence)\n","        \n","        #smallest_index = [index for index,value in enumerate(sample_confidence) if value==min(sample_confidence)]\n","        #find the elements with lowest/highest active scores\n","        smallest_index = []\n","        selected_num = 2 #2 3\n","        if method == \"random_sampling\":\n","          current_active_indice = [i for i in range(len(active_indice))];\n","          if len(current_active_indice)<selected_num:\n","            smallest_index = random.sample(current_active_indice, len(current_active_indice))\n","          else:\n","            smallest_index = random.sample(current_active_indice, selected_num)\n","        elif method == \"margin_sampling\":\n","          sorts = np.argsort(sample_confidence)\n","          if len(sorts) < selected_num:\n","            smallest_index = sorts\n","          else:\n","            smallest_index = sorts[0:selected_num]\n","        elif method == \"least_confidence\":\n","          sorts = np.argsort(-sample_confidence)\n","          if len(sorts) < selected_num:\n","            smallest_index = sorts\n","          else:\n","            smallest_index = sorts[0:selected_num]\n","        elif method == \"entropy_sampling\":\n","          sorts = np.argsort(-sample_confidence)\n","          if len(sorts) < selected_num:\n","            smallest_index = sorts\n","          else:\n","            smallest_index = sorts[0:selected_num]\n","        elif method == \"excepted_gradient_length\":\n","          sorts = np.argsort(-sample_confidence)\n","          if len(sorts) < selected_num:\n","            smallest_index = sorts\n","          else:\n","            smallest_index = sorts[0:selected_num]\n","        \n","        print(smallest_index)\n","        \n","        smallest_elements = [active_X_train[i] for i in smallest_index]\n","        smallest_labels = [active_Y_train[i] for i in smallest_index]\n","\n","        reshaped_elements = []\n","        reshaped_labels = []\n","        for tuning_item, tuning_label in zip(smallest_elements, smallest_labels):\n","          item = [tuning_item]\n","          label = [tuning_label]\n","          tuning_item = resize(item,resize_width,resize_height)\n","          tuning_label = resize(label,resize_width,resize_height)\n","\n","          tuning_item = np.array(tuning_item)\n","          tuning_label = np.array(tuning_label)\n","\n","          #smallest_elements, smallest_labels = extract_random(smallest_elements, smallest_labels, dx, dx, unlabeled_patches_num*len(smallest_index))\n","          \n","          temp_elements = []\n","          temp_labels = []\n","          for a, b in zip(tuning_item, tuning_label):\n","            for i in range(resize_height//dx):\n","              for j in range(resize_width//dx):\n","                temp_elements.append(a[i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n","                temp_labels.append(b[i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n","\n","          temp_elements = np.array(temp_elements)[:,np.newaxis,...,np.newaxis]\n","          temp_labels = np.array(temp_labels)[:,...,np.newaxis]\n","\n","          reshaped_elements.append(temp_elements)\n","          reshaped_labels.append(temp_labels)\n","\n","        smallest_elements = np.concatenate(reshaped_elements, axis=0)\n","        smallest_labels = np.concatenate(reshaped_labels, axis=0)\n","        print(smallest_elements.shape)\n","        print(smallest_labels.shape)\n","\n","        #smallest_elements = smallest_elements[..., np.newaxis]\n","\n","        # change the shape of Y_train, make it become (2880,2304), keep the first dimension\n","        # unchanged, merge other dimensions\n","        smallest_labels = smallest_labels.reshape(smallest_labels.shape[0],-1)\n","        print('smallest_labels shape: '+str(smallest_labels.shape))\n","        # increase one dimension, make it become (2880,2304,1)\n","        smallest_labels = smallest_labels[..., np.newaxis]\n","        print('Y_train shape: '+str(Y_train.shape))\n","        temp = 1 - smallest_labels\n","        # make it become (2880,2304,2)\n","        smallest_labels = np.concatenate([smallest_labels, temp], axis=2)\n","\n","        print(smallest_elements.shape)\n","        print(smallest_labels.shape)          \n","        \n","\n","        new_train_samples = []\n","        for i in range(final_new_train_samples.shape[0]):\n","          ori_element = final_new_train_samples[i,...]\n","          new_train_samples.append(ori_element[np.newaxis,:])\n","        for j in range(smallest_elements.shape[0]):\n","          new_element = smallest_elements[j,...]\n","          new_train_samples.append(new_element[np.newaxis,:])\n","        final_new_train_samples = np.concatenate(new_train_samples,axis=0)\n","        \n","        new_train_labels = []\n","        for i in range(final_new_train_labels.shape[0]):\n","          ori_label = final_new_train_labels[i,...]\n","          new_train_labels.append(ori_label[np.newaxis,:])\n","        for j in range(smallest_labels.shape[0]):\n","          new_label = smallest_labels[j,...]\n","          new_train_labels.append(new_label[np.newaxis,:])\n","        final_new_train_labels = np.concatenate(new_train_labels,axis=0)\n","\n","        print('final_new_train_samples shape: '+str(final_new_train_samples.shape))\n","        print('final_new_train_labels shape: '+str(final_new_train_labels.shape))\n","      \n","        # use these 100 the most uncertain samples \n","        # to do the fine-tuning to this network\n","        print(\"Use the pre trained model for fine tuning\")\n","        print(model.layers)\n","\n","        for layer in model.layers[:10]:\n","          layer.trainable = False\n","          #print(layer)\n","          #print(\"these layers won't be trained\")\n","        for layer in model.layers[21:30]:\n","          layer.trainable = False\n","          #print(layer)\n","          #print(\"these layers won't be trained\")\n","        \n","        print(len(model.layers))\n","\n","        #set early stopping to prevent overfit\n","        #early_stopping = EarlyStopping(monitor='val_loss',patience=3,verbose=2)\n","\n","        #model.summary()  # output the calculation process of parameter Param\n","        checkpointer = ModelCheckpoint(filepath='/content/drive/MyDrive/MyProject/'+dataset+'/training_records/'+dataset+'_best_weights('+method+'_'+str(selected_item)+\n","                                      ').h5',verbose=1,monitor='accuracy',mode='auto',save_best_only=True)\n","        model.compile(optimizer=Adam(lr=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])\n","        history2 = model.fit(final_new_train_samples,final_new_train_labels,batch_size=64,epochs=10,verbose=2,shuffle=True,validation_split=0.0,callbacks=[checkpointer,early_stopping]) #,early_stopping\n","\n","        #update corresponding parameters\n","        ruleout_list = [active_indice[i] for i in smallest_index]\n","        active_indice = [x for x in active_indice if x not in ruleout_list]\n","        print(active_indice)\n","        active_samples_num = len(active_indice)*patches_num\n","\n","        # draw corresponding loss figures\n","        history1.history['accuracy'].extend(history2.history['accuracy'])\n","        history1.history['loss'].extend(history2.history['loss'])\n","        \n","      #print(history1.history['accuracy'])\n","      #print(history1.history['loss'])\n","\n","      #mode wb+ readable and writable, and files will be automatically created if they don't exist\n","      #binary mode\n","      with open('/content/drive/MyDrive/MyProject/'+dataset+'/training_records/'+method+'_'+str(selected_item)+'.txt','wb+') as file_pi:\n","        pickle.dump(history1.history, file_pi)\n","      \"\"\""],"execution_count":null,"outputs":[]}]}